\documentclass[11pt]{article}
\usepackage [french]{babel}
\usepackage [T1]{fontenc}

\usepackage[linesnumbered, ruled, french, onelanguage]{algorithm2e}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{adjustbox}%Permet de centrer les figures dans la largeur de la page même si les figures sont plus larges que \textwidth
\usepackage{graphicx}
\usepackage{placeins}%pour utiliser FloatBarrier afin que les figure respectent bien leur position dans le code
\usepackage{gensymb}%pour pouvoir écrire le signe °



\author{}
\title{Ray-Tracer}
\date{}

\begin{document}
\maketitle

\section{Détail des parties techniques}
\subsection{Le Ray Tracer}
\subsubsection{L'algorithme de base}
Dans notre vie quotidienne, nous voyons ce qui nous entoure grâce aux photons émis par les différentes sources de lumière présentent autour de nous (le soleil ou nos lumières artificielles notamment). Chaque photon émit par ces sources vient alors frapper un des objets qui fait notre environnement. Si l'on imagine par exemple une tasse en face de nous, avec une lampe au plafond, un photon partant de cette dernière viendra rebondir sur la tasse pour ensuite, éventuellement, être redirigé en direction de nos yeux. Si le photon arrive en effet jusqu'à nos yeux, c'est alors qu'on verra se dessiner la tasse devant nous. La tasse est éclairée et nous la voyons. Bien sûr, ce phénomène est, à l'échelle de l'humain, instantané et c'est pour cela que nous ne voyons pas notre environnement apparaître progressivement devant nous chaque fois que nous allumons notre lumière. \\
L'objectif d'un algorithme de ray-tracing est de calculer le rendu d'une scène 3D d'une façon similaire à celle qui nous permet de voir tous les jours. En effet, nous pouvons faire un parallèle entre notre algorithme de ray-tracing et la vie à laquelle nous sommes habitués grâce à 3 composants principaux:
\begin{itemize}
	\item{Une scène composée d'objets (sphères, plans, triangles, ...) que l'on peut comparer à notre environnement}
	\item{Une (ou plusieures sources de lumière) jouant le même rôle que nos sources de lumière habituelles}
	\item{Une caméra représentant nos yeux}
\end{itemize}
Une différence notable avec ce qui nous permet de voir dans la vie de tous les jours est que nos rayons (ou photons) partiront de la caméra (nos yeux) plutôt que de la source de lumière. Nous faisons en quelque sorte le chemin inverse. En lançant un rayon au travers de chaque pixel de l'image que nous voulons calculer, nous pourrons ensuite vérifier si le rayon a rencontré un objet de la scène. Dans ce cas, le pixel pourra alors être coloré de la couleur de l'objet, le rendant ainsi visible.

\begin {algorithm}[H]
	\DontPrintSemicolon
	\KwIn{\textit{height} la hauteur de l'image à rendre,\\\textit{width} la largeur de l'image,\\\textit{scene} la scène 3D à rendre}
	\KwOut{P un ensemble de pixels $p_{i, j}$ $\{p_{0, 0}, p_{0, 1}, \ldots, p_{height-1, width-1}\}$, $i$, $j \in \mathbb{N}$, $0 \leqslant i \leqslant height-1$, $0 \leqslant j \leqslant width-1$\\\hfill\\}

	$O \gets Origine\ de\ la\ caméra$\\
	\For {$y \gets 0$ \textbf{to} $height$} 
	{
		\For {$x \gets 0$ \textbf{to} $width$} 
		{
			$C_{y, x} \gets convPxCoToWorldCoords(x, y)$\\
			$\overrightarrow{ray} \gets \overrightarrow{OC_{y,x}}$\\
			\hfill\\
			$p_{y, x} \gets lancerRayon(\overrightarrow{ray}, scene)$
		}
	}

	\caption{Pseudo-code du lancer des rayons}
	\label{lancerRayons}
\end {algorithm}

Comme le montre l'algorithme \ref{lancerRayons}, parcourir chaque pixel ne suffit pas. Si l'on souhaite lancer un rayon depuis la caméra à travers le pixel que l'on chercher à colorer, il nous faut les coordoonnées de ce pixel exprimées dans l'espace 3D de la scène.\\
Prenons l'exemple du rendu d'une image dans une résolution 800x600 pixels. Si l'on veut calculer la couleur du pixel de coordonnées $P = (430, 256)$ sur l'image, construire un rayon d'origine $O$ la caméra et de direction le vecteur $\overrightarrow{d} = \overrightarrow{OP}$, n'est pas une solution viable. En effet, le point $P$ n'est pas exprimé dans les coordonnées de notre scène où se trouvent nos objets. Il est exprimé dans les coordonnées de l'image. Nous avons donc besoin de faire correspondre les coordoonnées d'un pixel de l'image à ses coordoonnées dans le monde:

\begin {algorithm}[H]
	\DontPrintSemicolon
	\KwIn{$x, y \in \mathbb{N}$, les coordonnées du pixel sur l'image,\\
		$width, height$\ la\ largeur\ et\ la\ hauteur\ de\ l'image\ à\ rendre,\\
		$F \in\ ]0; 180[$\ le\ champ\ de\ vision\ de\ la\ caméra\ (FOV)\ en\ degré}
	\KwOut{$P_{world}(x_{world}, y_{world}, -1)$\\le\ point\ dont\ les\ coordonnées\ représentent\ celles\ du\ pixel\ exprimées\ dans\ l'espace\ de\ la\ scène.\\\hfill\\}

	$aspectRatio \gets width/height$\\
	$F_{rad/2} \gets \frac{F}{2}*\frac{\pi}{180}$\\
	$demiPlaneHeight \gets tan(F_{rad/2})$\\
	\hfill\\
	$x_{world} \gets x$\\
	$y_{world} \gets y$\\
	\hfill\\
	$x_{world} \gets (x_{world} +0.5) / width$ 		\noindent\hspace{3cm}{\textbf{//Etape 1}}\\
	$x_{world} \gets x_{world} * 2 -1$ 			\noindent\hspace{4.18cm}{\textbf{//Etape 2}}\\
	$x_{world} \gets x_{world} * demiPlaneHeight$ 	\noindent\hspace{1.95cm}{\textbf{//Etape 3}}\\
	$x_{world} \gets x_{world} * aspectRatio$		\noindent\hspace{3.05cm}{\textbf{//Etape 4}}\\

	\hfill\\
	$y_{world} \gets (y_{world} +0.5) / height$ 		\noindent\hspace{3cm}{\textbf{//Etape 1}}\\
	$y_{world} \gets 1 - y_{world} * 2$ 			\noindent\hspace{4.3cm}{\textbf{//Etape 2}}\\
	$y_{world} \gets y_{world} * demiPlaneHeight$ 	\noindent\hspace{2.06cm}{\textbf{//Etape 3}}\\

	\hfill\\
	\Return{$(x_{world}, y_{world}, -1)$}

	\caption{Conversion des coordonnées d'un pixel de l'image aux coordonnées de la scène}
	\label{conversionPixel}
\end{algorithm}

Il est important de remarquer que notre caméra se trouve initialement aux coordonnées $O = (0, 0, 0)$ et que sa direction de regard est $\overrightarrow{d_{cam}} = (0, 0, -1)$. Par convention, la grille de pixel virtuelle (le plan de la caméra) à travers laquelle nous faisons passer nos rayons se trouve à 1 unité de distance de la caméra, dans sa direction de regard. Ainsi, la coordonnée z des pixels sera toujours -1.\\
\begin{figure}[h!]
	\adjustbox{center}{\includegraphics[width=1.4\textwidth]{img/rt/repCam2Rayonsv2.png}}

	\caption{Représentation de la caméra, de son plan, de sa direction ainsi que d'une sphère représentant l'environnement. Deux rayons sont également tracés.}
	\label{repreCamRayon}
\end{figure}
\FloatBarrier

En reprenant l'exemple de notre image de rendu en 800x600 pixels et en suivant les étapes de l'algorithme \label{conversionPixel}:\\
$P(x, y, z), x \in [0; 799], y \in [0; 599], z = -1$ :
\begin{enumerate}
	\item{\textbf{Etape 1} : On ajoute 0.5 à la valeur du pixel que l'on veut rendre. Les pixels étant des carrés de 1x1 unité sur l'image, ajouter 0.5 fera donc passer le rayon au centre du pixel. La division par la largeur et la hauteur de l'image (pour x et y respectivement) permet de ramener les coordonnées des pixels dans l'intervalle $[0; 1]$.\\
		\hfill\\
	         	$P(x, y, z), x \in [0; 1], y \in[0; 1], z = -1$}
	\item{\textbf{Etape 2} : Le but de cette étape est de ramener les pixels dans l'intervalle $[-1; 1]$.\\
		$x*2 \Rightarrow x \in [0; 2]$
		$\\x - 1 \Rightarrow x \in [-1; 1]$\\
		Même raisonnement pour y à une inversion d'opération prêt.\\
		\hfill\\
	          	$P(x, y, z), x \in [-1; 1], y \in[-1; 1], z = -1$}
	\item{\textbf{Etape 3} : Cette étape permet de prendre en compte le champ de vision de la caméra. Plus le champ de vision se rapproche de 180\degree, plus la caméra voit une grande partie de la scène et inversement. Le champ de vision peut alors donner une impression de "zoom". Voir plus bas la figure \ref{fovCam}\\\hfill\\
		$P(x,y , z), x \in [-1; 1]*FOV, y \in [-1; 1]*FOV, z = -1$.}\\
	\item{\textbf{Etape 4} : Jusqu'alors, les coordonnées de nos pixels converties dans l'espace du monde formaient un plan de caméra carré. Comment faire si l'image que nous voulons rendre n'est pas elle-même carrée (ce qui est très fréquent) ? Il nous faut prendre en compte le format de l'image donné par $width/height$. En multipliant la coordonnée x par ce format, nous "étirons" l'image ou la rétrécissons selon sa largeur. Le plan de la caméra respecte maintenant le même format que l'image que nous voulons rendre.\\\hfill\\
		$P(x,y , z), x \in [-1; 1]*FOV*aspectRatio, y \in [-1; 1]*FOV, z = -1$.}
\end{enumerate}

\begin{figure}[h!]
	\includegraphics{img/rt/fovCamera.png}

	\label{fovCam}
	\caption{En augmentant le FOV (champ de vision) de la caméra, l'angle $\alpha$ augmente. La hauteur du plan de la caméra, $BC$, est étirée. La caméra verra donc une plus grande partie de la scène.\break Source: scratchapixel.com}
\end{figure}
\FloatBarrier

A l'issue de ces implémentations, nous obtenons alors l'image suivante (la scène est composée d'une unique sphère rouge et d'un fond gris foncé):

\begin{figure}[h!]
	\adjustbox{center}{\includegraphics[width=\textwidth]{img/rt/basicRender.png}}

	\label{basicRender}
	\caption{Rendu le plus simple. Une sphère sans aucun effet d'ombrage}
\end{figure}
\FloatBarrier

Chaque rayon qui a été lancé et qui a intersecté la sphère a alors donné un pixel rouge. Les autres rayons, ceux qui n'ont pas intersecté la sphère, ont simplement renvoyé la couleur du fond de la scène. 

\subsubsection{L'ombrage de Phong}

Comme vu précédemment, la première image (fig. \ref{basicRender}) produite par notre algorithme n'est pas tout à fait satisfaisante. Il n'y a en effet aucun effet de profondeur ou de lumière. Difficile de savoir qu'on a une sphère devant nous et pas un simple disque plat. La prochaine étape est donc celle là de l'ombrage de Phong. Un tel ombrage se découpe en trois composantes/parties:
\begin{itemize}
	\item{La composante ambiante. Elle est responsable de la quantité minimale de lumière que va reçevoir un objet}
	\item{La composante diffuse. Cette composante est celle qui permet de donner à un objet son relief. Plus les rayons de la lumière arrivent perpendiculairement à la surface de l'objet, plus l'intensité lumineuse sur la surface de l'objet sera importante}
	\item{La composante spéculaire. Elle permet d'ajouter un effet "brillant" aux objets en fonction de la direction qu'empruntent les rayons réfléchis par la surface de l'objet par rapport à la caméra.}
\end{itemize}

La composante ambiante permet de donner une illumination de base à l'objet. Elle est très simple à calculer:\\

\begin{center}
	$Ambiante = I_L*C_A$
\end{center}
Avec $I_L\ \in\ [0;1]$ l'intensité de la source lumineuse,\\
$C_A\ \in\ [0; 1]$ le coefficient de luminosité ambiante de l'objet.

\subsubsection{Les réflexions}
\subsubsection{Les réfractions}

\section{Structuration du projet}
\subsection{Les packages}
\subsubsection{Liés au Ray Tracer}




\end{document}